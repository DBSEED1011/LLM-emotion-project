This folder contains the main execution script ("run_parallel.py"), a notebook for output checking ("check.ipynb"), a prompt documentation file ("Examples of prompts under different conditions.docx"), the "prompt" folder and result folders generated by running the script.

The "prompt" folder contains the main materials for constructing the LLM experiments, including the human demographic information file "demographic data.xlsx", the actual human experiment settings "Emo&TPP data.xlsx", the LLM settings "exp_model_class.py", and a general prompt file "person_all_game_prompt.json". The remaining files are intermediate files generated by running "run_parallel.py".

For each LLM agent, "run_parallel.py" generates five files in the "prompt" folder and creates corresponding result folders for each condition, all prefixed with "result_". Each folder contains text files for each agent, numbered from 0.

To verify the outputs, please run "check.ipynb", which checks the generated text files and produces merged data files.

If you need to adjust the number of generated subjects, modify the subnum_list in "run_parallel.py" (the upper limit should not exceed 1017, as there are only this many human participants). To test different experimental conditions (e.g., not reporting emotions, prompts without demographic information, or adjusting the temperature), make the corresponding modifications in "multi_round_person.py". The scripts "generate_character_prompt.py" and "generate_game_setting_prompt.py" are used to generate the intermediate files containing the experimental setting information.

The original code was written by Yujia Zhou and subsequently revised and managed by Haotian Tan and Yiqing Dai. To validate the code, we ran two representative agents for four LLMs under the conditions "prompt with persona", "experiment with emotion self-report", and "Temperature == 1". "merged_all_models_persona_emotion_1.0.txt" is the merged data file (all four LLMs). Note that it is only an example with two agents for each LLM. This validation data is entirely independent of the main analysis and was not used elsewhere. The validation runs were conducted on October 10, 2025, by Haotian Tan.

Please note that the api_key has been removed from "multi_round_person.py", so the code cannot be executed as is. Anyone who wants to run the code must provide their own api_key.

The folder also contains Examples of prompts under different conditions.docx, which provides the explicit prompts used in the experiments. Only GPT-3.5 and DeepSeek-V3 examples are included, since the prompts for o3-mini and DeepSeek-R1 are identical to those of DeepSeek-V3. In addition, under the same prompt settings, DeepSeek-R1 still reports its reasoning process. We retained the reasoning outputs and analyzed them separately.